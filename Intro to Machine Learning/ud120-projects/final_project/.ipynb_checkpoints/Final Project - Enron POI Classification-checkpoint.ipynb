{
 "metadata": {
  "name": "",
  "signature": "sha256:bbd61bbf409231d218b54ea077925eb71a08b0c9dcbdf7fc630d696804c7cd6b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gensim\n",
      "import os\n",
      "import logging\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def iter_documents(top_directory):\n",
      "    '''\n",
      "    Generator: iterate over all relevant documents, yielding one\n",
      "    document(=list of utf8 tokens) at a time.\n",
      "    '''\n",
      "    # Find all .txt documents, no matter how deep under top_directory\n",
      "    for root, dirs, files in os.walk(top_directory):\n",
      "        #print root\n",
      "        #for fname in filter(lambda fname: fname.endswith('.txt'), files):\n",
      "        # replace with just file names since \n",
      "        # our files have no extension\n",
      "        for fname in files:\n",
      "            #print fname\n",
      "            # read each document as one big string\n",
      "            document = open(os.path.join(root, fname)).read()\n",
      "            #break document into utf8 tokens\n",
      "            yield gensim.utils.tokenize(document, lower=True, errors='ignore')\n",
      "\n",
      "class TxtSubdirsCorpus(object):\n",
      "    '''\n",
      "    iterable: on each iteration, return bag-of-words vectors,\n",
      "    one vector for each document.\n",
      "    \n",
      "    Process one document at a time using generators, never\n",
      "    load the entire corpus in RAM.\n",
      "    '''\n",
      "    def __init__(self, top_dir):\n",
      "        self.top_dir = top_dir\n",
      "        # create a dictionary = mapping for documents => sparse vectors\n",
      "        self.dictionary = gensim.corpora.Dictionary(iter_documents(top_dir))\n",
      "    \n",
      "    def __iter__(self):\n",
      "        '''\n",
      "        Again, __iter__ is a generator => TxtSubdirsCorpus is a streamed iterable.\n",
      "        '''\n",
      "        for tokens in iter_documents(self.top_dir):\n",
      "            # transforms tokens (strings) in a sparse vector, one at a time\n",
      "            yield self.dictionary.doc2bow(tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# that's it! the streamed corpus of sparse vectors is ready\n",
      "search_dir = r'C:\\Users\\fch80_000\\Temp2\\Intro to Machine Learning\\ud120-projects\\enron_mail_20110402\\maildir\\allen-p\\_sent_mail'\n",
      "corpus = TxtSubdirsCorpus(search_dir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print the corpus vectors\n",
      "#for vector in corpus:\n",
      "#    print vector"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#from gensim.models.lsimodel import stochastic_svd as svd\n",
      "#u, s = svd(corpus, rank=200, num_terms=len(corpus.dictionary), chunksize=5000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim import models\n",
      "tfidf = models.TfidfModel(corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec = [(0, 1), (4, 1)]\n",
      "print(tfidf[vec])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0, 0.6918996341898515), (4, 0.7219936954073419)]\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim import similarities\n",
      "index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=12)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sims = index[tfidf[vec]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(list(enumerate(sims)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}