{
 "metadata": {
  "name": "",
  "signature": "sha256:f5edb7ffc68cdd32b9390b1ca5efa271b0ece5110e6a1ac144d2e985127e1162"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sw = stopwords.words('english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(sw)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "127"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem.snowball import SnowballStemmer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stemmer = SnowballStemmer('english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stemmer.stem('unresponsive')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "u'unrespons'"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%load ../ud120-projects/tools/parse_out_email_text.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%%writefile ../ud120-projects/tools/parse_out_email_text.py\n",
      "#!/usr/bin/python\n",
      "\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "import string\n",
      "\n",
      "def parseOutText(f):\n",
      "    \"\"\" given an opened email file f, parse out all text below the\n",
      "        metadata block at the top\n",
      "        (in Part 2, you will also add stemming capabilities)\n",
      "        and return a string that contains all the words\n",
      "        in the email (space-separated) \n",
      "        \n",
      "        example use case:\n",
      "        f = open(\"email_file_name.txt\", \"r\")\n",
      "        text = parseOutText(f)\n",
      "        \n",
      "        \"\"\"\n",
      "\n",
      "\n",
      "    f.seek(0)  ### go back to beginning of file (annoying)\n",
      "    all_text = f.read()\n",
      "\n",
      "    ### split off metadata\n",
      "    content = all_text.split(\"X-FileName:\")\n",
      "    words = \"\"\n",
      "    if len(content) > 1:\n",
      "        ### remove punctuation\n",
      "        text_string = content[1].translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
      "\n",
      "        ### project part 2: comment out the line below\n",
      "        # words = text_string\n",
      "        \n",
      "        ### split the text string into individual words, stem each word,\n",
      "        ### and append the stemmed word to words (make sure there's a single\n",
      "        ### space between each stemmed word)\n",
      "        stemmer = SnowballStemmer('english')\n",
      "        \n",
      "        text_string = text_string.split()\n",
      "\n",
      "        words = [stemmer.stem(word).strip()for word in text_string if word != \"\"]\n",
      "        \n",
      "        words = \" \".join(words)\n",
      "        \n",
      "    return words\n",
      "\n",
      "    \n",
      "\n",
      "def main():\n",
      "    ff = open(\"../ud120-projects/text_learning/test_email.txt\", \"r\")\n",
      "    text = parseOutText(ff)\n",
      "    print text\n",
      "\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hi everyon if you can read this messag your proper use parseouttext pleas proceed to the next part of the project\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%load ../ud120-projects/text_learning/vectorize_text.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%%writefile ../ud120-projects/text_learning/vectorize_text.py\n",
      "#!/usr/bin/python\n",
      "\n",
      "import os\n",
      "import pickle\n",
      "import re\n",
      "import sys\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "\n",
      "sys.path.append( \"../ud120-projects/tools/\" )\n",
      "from parse_out_email_text import parseOutText\n",
      "\n",
      "\"\"\"\n",
      "    starter code to process the emails from Sara and Chris to extract\n",
      "    the features and get the documents ready for classification\n",
      "\n",
      "    the list of all the emails from Sara are in the from_sara list\n",
      "    likewise for emails from Chris (from_chris)\n",
      "\n",
      "    the actual documents are in the Enron email dataset, which\n",
      "    you downloaded/unpacked in Part 0 of the first mini-project\n",
      "\n",
      "    the data is stored in lists and packed away in pickle files at the end\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "from_sara  = open(\"../ud120-projects/text_learning/from_sara.txt\", \"r\")\n",
      "from_chris = open(\"../ud120-projects/text_learning/from_chris.txt\", \"r\")\n",
      "\n",
      "from_data = []\n",
      "word_data = []\n",
      "\n",
      "### temp_counter is a way to speed up the development--there are\n",
      "### thousands of emails from Sara and Chris, so running over all of them\n",
      "### can take a long time\n",
      "### temp_counter helps you only look at the first 200 emails in the list\n",
      "temp_counter = 0\n",
      "text = \"\"\n",
      "\n",
      "# sw = stopwords.words('english')\n",
      "\n",
      "for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
      "    for path in from_person:\n",
      "        ### only look at first 200 emails when developing\n",
      "        ### once everything is working, remove this line to run over full dataset\n",
      "        #temp_counter += 1\n",
      "        #if temp_counter < 200:\n",
      "            path = os.path.join('../ud120-projects/', path[:-1])\n",
      "            #print path\n",
      "            email = open(path, \"r\")\n",
      "            ### use parseOutText to extract the text from the opened email\n",
      "            text = parseOutText(email)\n",
      "            # print text\n",
      "            ### use str.replace() to remove any instances of the words\n",
      "            remove_these = [\"sara\", \"shackleton\", \"chris\", \"germani\", \"sshacklensf\", \"cgermannsf\"]\n",
      "            for word in remove_these:\n",
      "                text = text.replace(word, \"\")\n",
      "            \n",
      "            # remove nltk stopwords:    \n",
      "            # text = ' '.join([word for word in text.split() if word not in sw])\n",
      "            \n",
      "            ### append the text to word_data\n",
      "            word_data.append(text)\n",
      "            if name == 'sara':\n",
      "                from_data.append(0)\n",
      "            elif name == 'chris':\n",
      "                from_data.append(1)\n",
      "                \n",
      "            #print word_data\n",
      "            #print from_data\n",
      "            ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
      "            email.close()\n",
      "\n",
      "print \"emails processed\"\n",
      "from_sara.close()\n",
      "from_chris.close()\n",
      "\n",
      "pickle.dump( word_data, open(\"../ud120-projects/text_learning/your_word_data.pkl\", \"w\") )\n",
      "pickle.dump( from_data, open(\"../ud120-projects/text_learning/your_email_authors.pkl\", \"w\") )\n",
      "\n",
      "\n",
      "### in Part 4, do TfIdf vectorization here\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "emails processed\n"
       ]
      }
     ],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = pickle.load(open(\"../ud120-projects/text_learning/your_word_data.pkl\"))\n",
      "print s[152]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tjonesnsf stephani and sam need nymex calendar\n"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = pickle.load(open(\"../ud120-projects/text_learning/your_email_authors.pkl\"))\n",
      "print s[16000:16060]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#from sklearn.feature_extraction.text import TfidfTransformer\n",
      "#from sklearn.feature_extraction.text import CountVectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#vectorizer = CountVectorizer()\n",
      "#bow = vectorizer.fit(word_data)\n",
      "#bow = vectorizer.transform(word_data)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#transformer = TfidfTransformer()\n",
      "#tfidf = transformer.fit_transform(bow)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#tfidf.toarray()  \n",
      "word_data[152]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 65,
       "text": [
        "u'tjonesnsf stephani and sam need nymex calendar'"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "vect = TfidfVectorizer(min_df=1, stop_words='english')\n",
      "tdidf = vect.fit_transform(word_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tdidf2.toarray()\n",
      "len(vect.get_feature_names())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 67,
       "text": [
        "38757"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vect.get_feature_names()[34597]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 68,
       "text": [
        "u'stephaniethank'"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}