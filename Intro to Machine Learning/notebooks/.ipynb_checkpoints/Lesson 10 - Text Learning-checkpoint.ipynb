{
 "metadata": {
  "name": "",
  "signature": "sha256:65a66f77fb178364a4ff0cc05d0c9e1cdd76990cd510433a4357bbe4445ca5ad"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sw = stopwords.words('english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(sw)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "127"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem.snowball import SnowballStemmer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stemmer = SnowballStemmer('english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stemmer.stem('unresponsive')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "u'unrespons'"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%load ../ud120-projects/tools/parse_out_email_text.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%%writefile ../ud120-projects/tools/parse_out_email_text.py\n",
      "#!/usr/bin/python\n",
      "\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "import string\n",
      "\n",
      "def parseOutText(f):\n",
      "    \"\"\" given an opened email file f, parse out all text below the\n",
      "        metadata block at the top\n",
      "        (in Part 2, you will also add stemming capabilities)\n",
      "        and return a string that contains all the words\n",
      "        in the email (space-separated) \n",
      "        \n",
      "        example use case:\n",
      "        f = open(\"email_file_name.txt\", \"r\")\n",
      "        text = parseOutText(f)\n",
      "        \n",
      "        \"\"\"\n",
      "\n",
      "\n",
      "    f.seek(0)  ### go back to beginning of file (annoying)\n",
      "    all_text = f.read()\n",
      "\n",
      "    ### split off metadata\n",
      "    content = all_text.split(\"X-FileName:\")\n",
      "    words = \"\"\n",
      "    if len(content) > 1:\n",
      "        ### remove punctuation\n",
      "        text_string = content[1].translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
      "\n",
      "        ### project part 2: comment out the line below\n",
      "        # words = text_string\n",
      "        \n",
      "        ### split the text string into individual words, stem each word,\n",
      "        ### and append the stemmed word to words (make sure there's a single\n",
      "        ### space between each stemmed word)\n",
      "        stemmer = SnowballStemmer('english')\n",
      "        \n",
      "        text_string = text_string.split()\n",
      "\n",
      "        words = [stemmer.stem(word).strip()for word in text_string if word != \"\"]\n",
      "        \n",
      "        words = \" \".join(words)\n",
      "        \n",
      "    return words\n",
      "\n",
      "    \n",
      "\n",
      "def main():\n",
      "    ff = open(\"../ud120-projects/text_learning/test_email.txt\", \"r\")\n",
      "    text = parseOutText(ff)\n",
      "    print text\n",
      "\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hi everyon if you can read this messag your proper use parseouttext pleas proceed to the next part of the project\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%load ../ud120-projects/text_learning/vectorize_text.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%%writefile ../ud120-projects/text_learning/vectorize_text.py\n",
      "#!/usr/bin/python\n",
      "\n",
      "import os\n",
      "import pickle\n",
      "import re\n",
      "import sys\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "\n",
      "sys.path.append( \"../ud120-projects/tools/\" )\n",
      "from parse_out_email_text import parseOutText\n",
      "\n",
      "\"\"\"\n",
      "    starter code to process the emails from Sara and Chris to extract\n",
      "    the features and get the documents ready for classification\n",
      "\n",
      "    the list of all the emails from Sara are in the from_sara list\n",
      "    likewise for emails from Chris (from_chris)\n",
      "\n",
      "    the actual documents are in the Enron email dataset, which\n",
      "    you downloaded/unpacked in Part 0 of the first mini-project\n",
      "\n",
      "    the data is stored in lists and packed away in pickle files at the end\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "from_sara  = open(\"../ud120-projects/text_learning/from_sara.txt\", \"r\")\n",
      "from_chris = open(\"../ud120-projects/text_learning/from_chris.txt\", \"r\")\n",
      "\n",
      "from_data = []\n",
      "word_data = []\n",
      "\n",
      "### temp_counter is a way to speed up the development--there are\n",
      "### thousands of emails from Sara and Chris, so running over all of them\n",
      "### can take a long time\n",
      "### temp_counter helps you only look at the first 200 emails in the list\n",
      "temp_counter = 0\n",
      "text = \"\"\n",
      "\n",
      "# sw = stopwords.words('english')\n",
      "\n",
      "for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
      "    for path in from_person:\n",
      "        ### only look at first 200 emails when developing\n",
      "        ### once everything is working, remove this line to run over full dataset\n",
      "        #temp_counter += 1\n",
      "        #if temp_counter < 200:\n",
      "            path = os.path.join('../ud120-projects/', path[:-1])\n",
      "            #print path\n",
      "            email = open(path, \"r\")\n",
      "            ### use parseOutText to extract the text from the opened email\n",
      "            text = parseOutText(email)\n",
      "            # print text\n",
      "            ### use str.replace() to remove any instances of the words\n",
      "            remove_these = [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
      "            for word in remove_these:\n",
      "                text = text.replace(word, \"\")\n",
      "            \n",
      "            # remove nltk stopwords:    \n",
      "            # text = ' '.join([word for word in text.split() if word not in sw])\n",
      "            \n",
      "            ### append the text to word_data\n",
      "            word_data.append(text)\n",
      "            if name == 'sara':\n",
      "                from_data.append(0)\n",
      "            elif name == 'chris':\n",
      "                from_data.append(1)\n",
      "                \n",
      "            #print word_data\n",
      "            #print from_data\n",
      "            ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
      "            email.close()\n",
      "\n",
      "print \"emails processed\"\n",
      "from_sara.close()\n",
      "from_chris.close()\n",
      "\n",
      "pickle.dump( word_data, open(\"your_word_data.pkl\", \"w\") )\n",
      "pickle.dump( from_data, open(\"your_email_authors.pkl\", \"w\") )\n",
      "\n",
      "\n",
      "### in Part 4, do TfIdf vectorization here\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "emails processed\n"
       ]
      }
     ],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = pickle.load(open(\"your_word_data.pkl\"))\n",
      "print s[0:2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'sbaile2 nonprivilegedpst susan pleas send the forego list to richard thank   enron wholesal servic 1400 smith street eb3801a houston tx 77002 ph 713 8535620 fax 713 6463490', u'sbaile2 nonprivilegedpst 1 txu energi trade compani 2 bp capit energi fund lp may be subject to mutual termin 2 nobl gas market inc 3 puget sound energi inc 4 virginia power energi market inc 5 t boon picken may be subject to mutual termin 5 neumin product co 6 sodra skogsagarna ek for probabl an ectric counterparti 6 texaco natur gas inc may be book incorrect for texaco inc financi trade 7 ace capit re oversea ltd 8 nevada power compani 9 prior energi corpor 10 select energi inc origin messag from tweed sheila sent thursday januari 31 2002 310 pm to   subject pleas send me the name of the 10 counterparti that we are evalu thank']\n"
       ]
      }
     ],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = pickle.load(open(\"your_email_authors.pkl\"))\n",
      "print s[0:2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 0]\n"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#from sklearn.feature_extraction.text import TfidfTransformer\n",
      "#from sklearn.feature_extraction.text import CountVectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#vectorizer = CountVectorizer()\n",
      "#bow = vectorizer.fit(word_data)\n",
      "#bow = vectorizer.transform(word_data)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#transformer = TfidfTransformer()\n",
      "#tfidf = transformer.fit_transform(bow)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "__init__() got an unexpected keyword argument 'stop_words'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-100-3bc7cc6fa398>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtransformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'stop_words'"
       ]
      }
     ],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#tfidf.toarray()   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 84,
       "text": [
        "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
        "       ..., \n",
        "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
       ]
      }
     ],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "vect = TfidfVectorizer(min_df=1, stop_words='english')\n",
      "tdidf = vect.fit_transform(word_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tdidf2.toarray()\n",
      "len(vect.get_feature_names())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 105,
       "text": [
        "38757"
       ]
      }
     ],
     "prompt_number": 105
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vect.get_feature_names()[34597]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 111,
       "text": [
        "u'stephaniethank'"
       ]
      }
     ],
     "prompt_number": 111
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}